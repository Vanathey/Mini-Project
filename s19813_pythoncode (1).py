# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15HccLmIuf41OE2ciIzRXLTlJeO0J8H1m
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/employee_performance_wellbeing_1.csv')

missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])

summary_info = df.info()
summary_stats = df.describe(include='all')

plt.figure(figsize=(10, 5))
sns.histplot(df['Health_Score'], kde=True)
plt.title("Distribution of Health Score")
plt.xlabel("Health Score")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
sns.boxplot(x='Stress_Level', y='Health_Score', data=df)
plt.title("Health Score by Stress Level")
plt.tight_layout()
plt.show()

numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
correlation_matrix = df[numerical_cols].corr()

plt.figure(figsize=(12, 7))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title("Correlation Matrix of Numerical Features")
plt.tight_layout()
plt.show()

summary_info, summary_stats

"""Principal Component Analysis (PCA)

"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

features = ['Age', 'Work_Hours', 'Sleep_Hours', 'Health_Score',
            'Absenteeism_Days', 'Years_at_Company', 'Mental_Health_Score']
X = df[features].dropna()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA()
X_pca = pca.fit_transform(X_scaled)

explained = pca.explained_variance_ratio_
cum_explained = np.cumsum(explained)

plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained) + 1), cum_explained, marker='o')
plt.title("Cumulative Explained Variance by PCA Components")
plt.xlabel("Principal Component")
plt.ylabel("Cumulative Variance Explained")
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained) + 1), explained, marker='o', linestyle='-')
plt.title("Scree Plot (Eigenvalues of Principal Components)")
plt.xlabel("Principal Component")
plt.ylabel("Eigenvalue")
plt.grid(True)
plt.tight_layout()
plt.show()

for i, var in enumerate(explained, 1):
    print(f"PC{i}: {var:.3f} explained variance")

loadings = pd.DataFrame(
    pca.components_.T,
    columns=[f'PC{i+1}' for i in range(len(features))],
    index=features
)

print("PCA Loading Matrix:")
print(loadings.round(3))

"""Factor Analysis (FA)"""

!pip install factor_analyzer
import pandas as pd
from sklearn.preprocessing import StandardScaler
from factor_analyzer import FactorAnalyzer, calculate_kmo, calculate_bartlett_sphericity
import matplotlib.pyplot as plt

# Select suitable numerical variables for FA
fa_vars = [
    "Work_Hours",
    "Sleep_Hours",
    "Health_Score",
    "Mental_Health_Score",
    "Absenteeism_Days",
    "Years_at_Company"
]
df_fa = df[fa_vars]

# Drop rows with missing values
df_fa.dropna(inplace=True)

# Bartlett‚Äôs Test of Sphericity and KMO test
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo

chi_square_value, p_value = calculate_bartlett_sphericity(df_fa)
kmo_all, kmo_model = calculate_kmo(df_fa)

print("Bartlett‚Äôs Test p-value:", p_value)
print("KMO Overall Score:", kmo_model)

# Determine number of factors using eigenvalues
fa = FactorAnalyzer(n_factors=len(fa_vars), rotation=None)
fa.fit(df_fa)
ev, v = fa.get_eigenvalues()

# Scree plot
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(ev)+1), ev, marker='o')
plt.axhline(y=1, color='r', linestyle='--')
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid(True)
plt.show()

# Fit FA with chosen number of factors (e.g., 2)
fa = FactorAnalyzer(n_factors=2, rotation='varimax')
fa.fit(df_fa)

# Show factor loadings
loadings = pd.DataFrame(fa.loadings_, index=fa_vars, columns=["Factor 1", "Factor 2"])
print(loadings)

fa = FactorAnalyzer(n_factors=3, rotation='varimax')
fa.fit(X_scaled)

loadings = pd.DataFrame(fa.loadings_, index=features, columns=["Factor1", "Factor2", "Factor3"])
print("\nFactor Loadings (Varimax Rotation):")
print(loadings.round(3))

"""Discriminant Analysis"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

target = 'Stress_Level'
features = ['Age', 'Work_Hours', 'Sleep_Hours', 'Health_Score',
            'Absenteeism_Days', 'Years_at_Company', 'Mental_Health_Score']

le = LabelEncoder()
df[target] = le.fit_transform(df[target])  # e.g., Low=0, Moderate=1, High=2

X = df[features]
y = df[target]

X = X.dropna()
y = y.loc[X.index]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)

y_pred = lda.predict(X_test)
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=le.classes_))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix for LDA")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

"""Canonical Correlation Analysis"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cross_decomposition import CCA
import numpy as np

print(df.columns)

# Select and encode predictor variables
X_vars = ['Work_Hours', 'Sleep_Hours', 'Years_at_Company', 'Physical_Activity']
Y_vars = ['Health_Score', 'Mental_Health_Score', 'Absenteeism_Days']

# Encode 'Physical_Activity' (ordinal encoding)
activity_map = {'None': 0, 'Light': 1, 'Moderate': 2, 'Intense': 3}
df['Physical_Activity_Encoded'] = df['Physical_Activity'].map(activity_map)

# Build final X and Y datasets
X = df[['Work_Hours', 'Sleep_Hours', 'Years_at_Company', 'Physical_Activity_Encoded']]
Y = df[Y_vars]

# Drop rows with missing values
combined = pd.concat([X, Y], axis=1).dropna()
X = combined[X.columns]
Y = combined[Y.columns]

# Standardize both sets
scaler = StandardScaler()
X_std = scaler.fit_transform(X)
Y_std = scaler.fit_transform(Y)

# Perform Canonical Correlation Analysis
cca = CCA(n_components=2)
X_c, Y_c = cca.fit_transform(X_std, Y_std)

# Print canonical correlations
import numpy as np
canonical_corrs = [np.corrcoef(X_c[:, i], Y_c[:, i])[0, 1] for i in range(2)]
print("Canonical Correlations:", canonical_corrs)

# Optionally, display loadings
x_loadings = pd.DataFrame(cca.x_weights_, index=X.columns, columns=['X_Canon1', 'X_Canon2'])
y_loadings = pd.DataFrame(cca.y_weights_, index=Y.columns, columns=['Y_Canon1', 'Y_Canon2'])

print("\nX Loadings:")
print(x_loadings)

print("\nY Loadings:")
print(y_loadings)

"""Structural Equation Modeling (SEM)"""

!pip install semopy
from semopy import Model
from sklearn.preprocessing import LabelEncoder

df['Job_Satisfaction'] = LabelEncoder().fit_transform(df['Job_Satisfaction'])
df['Remote_Work'] = LabelEncoder().fit_transform(df['Remote_Work'])

df_sem = df[['Work_Hours', 'Job_Satisfaction', 'Remote_Work',
             'Health_Score', 'Mental_Health_Score', 'Absenteeism_Days']].dropna()

model_desc = """
# Measurement model
Job_Engagement =~ Work_Hours + Job_Satisfaction + Remote_Work
Health_Outcome =~ Health_Score + Mental_Health_Score + Absenteeism_Days

# Structural model
Health_Outcome ~ Job_Engagement
"""

model = Model(model_desc)
res = model.fit(df_sem)

estimates = model.inspect()
print("üîç SEM Estimates:")
print(estimates)